# ğŸš€ Neural Networks from Scratch

ğŸ§  **Build Neural Networks Without Deep Learning Frameworks - Just NumPy!**

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://python.org)
[![NumPy](https://img.shields.io/badge/NumPy-Latest-orange.svg)](https://numpy.org)

## ğŸ“š Overview

This project demonstrates how to build neural networks from scratch using only NumPy, without relying on high-level deep learning frameworks. Perfect for understanding the fundamental concepts behind neural networks! 


## âœ¨ Features

- ğŸ—ï¸ **3-Layer Architecture**: Input â†’ Hidden â†’ Output layers
- ğŸ­ **Multiple Activation Functions**: ReLU and Sigmoid implementations
- ğŸš€ **Optimizers**: SGD and Momentum optimization
- ğŸ“ˆ **MNIST Dataset**: Train on handwritten digit recognition
- âš¡ **Fast Training**: ~98% accuracy in just 10 seconds on CPU
- ğŸ““ **Jupyter Notebook**: Interactive learning experience

## ğŸƒâ€â™‚ï¸ Quick Start

### Prerequisites
```bash
pip install numpy matplotlib
```

### ğŸš€ Basic Training
```bash
python train.py
```

### ğŸ›ï¸ Advanced Configuration
```bash
# Using sigmoid activation with momentum optimizer
python train.py --activation sigmoid --optimizer momentum --l_rate 4

# Custom batch size and learning rate
python train.py --batch_size 64 --l_rate 0.01 --beta 0.9
```

## âš™ï¸ Configuration Options

| Parameter | Options | Description |
|-----------|---------|-------------|
| `--activation` | `relu`, `sigmoid` | ğŸ­ Activation function |
| `--optimizer` | `sgd`, `momentum` | ğŸƒâ€â™‚ï¸ Optimization algorithm |
| `--batch_size` | Integer | ğŸ“¦ Training batch size |
| `--l_rate` | Float | ğŸ“ˆ Learning rate |
| `--beta` | Float | ğŸ¯ Beta parameter for momentum |

## ğŸ—ï¸ Architecture

### ğŸ”¢ Network Structure
- **Input Layer**: 784 nodes (28Ã—28 flattened images)
- **Hidden Layer**: 64 nodes  
- **Output Layer**: 10 nodes (digit classes 0-9)

### ğŸ§® Mathematical Foundation

**Forward Propagation:**
```
Zâ‚ = Wâ‚X + bâ‚
Aâ‚ = activation(Zâ‚)
Zâ‚‚ = Wâ‚‚Aâ‚ + bâ‚‚  
Aâ‚‚ = softmax(Zâ‚‚)
```

**Backward Propagation:**
```
âˆ‚L/âˆ‚W = (1/m) * A * X^T
âˆ‚L/âˆ‚b = (1/m) * Î£(âˆ‚L/âˆ‚Z)
```

## ğŸ“Š Dataset

ğŸ”¢ **MNIST Handwritten Digits**
- ğŸ“š 70,000 total images
- ğŸ‹ï¸ 60,000 training samples
- ğŸ§ª 10,000 testing samples
- ğŸ“ 28Ã—28 pixel grayscale images
- ğŸ¯ 10 classes (digits 0-9)

## ğŸš€ Performance

- âš¡ **Training Time**: ~10 seconds on CPU
- ğŸ¯ **Accuracy**: ~98% on test set
- ğŸ’¾ **Memory Efficient**: Pure NumPy implementation
- ğŸ”§ **Lightweight**: No external ML frameworks required

## ğŸ“ Project Structure

```
ğŸ“¦ neural-networks-scratch/
â”œâ”€â”€ ğŸ train.py              # Main training script
â”œâ”€â”€ ğŸ§  model.py              # Neural network implementation
â”œâ”€â”€ ğŸ““ NN-from-Scratch.ipynb # Interactive Jupyter notebook
â”œâ”€â”€ ğŸ“Š data/                 # Dataset directory
â”œâ”€â”€ ğŸ–¼ï¸ figs/                 # Visualization figures
â””â”€â”€ ğŸ“š README.md             # This file
```

## ğŸ“ Educational Value

### ğŸ”¬ Core Concepts Covered
- ğŸ§® **Matrix Operations**: Understanding dot products and dimensions
- ğŸ“ **Calculus**: Gradients and chain rule in backpropagation  
- ğŸ“Š **Statistics**: Weight initialization and normalization
- ğŸ¯ **Optimization**: Gradient descent and momentum

### ğŸ­ Activation Functions
```python
# ReLU Activation
def relu(x, derivative=False):
    if derivative:
        return np.where(x > 0, 1, 0)
    return np.maximum(0, x)

# Sigmoid Activation  
def sigmoid(x, derivative=False):
    if derivative:
        return sigmoid(x) * (1 - sigmoid(x))
    return 1 / (1 + np.exp(-x))
```

## ğŸ¨ Visualization

The project includes beautiful visualizations of:
- ğŸ“ˆ Training loss curves
- ğŸ¯ Accuracy progression  
- ğŸ” Weight distributions
- ğŸ§  Network architecture diagrams

## ğŸ”§ Advanced Usage

### ğŸ® Interactive Mode
```python
from model import DeepNeuralNetwork

# Initialize network
dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='relu')

# Train the model
dnn.train(x_train, y_train, x_test, y_test)

# Make predictions
predictions = dnn.predict(x_new)
```

### ğŸ“Š Custom Experiments
Try experimenting with:
- ğŸ—ï¸ Different architectures (more layers, different sizes)
- ğŸ­ New activation functions (Tanh, Leaky ReLU)
- ğŸš€ Advanced optimizers (Adam, RMSprop)
- ğŸ“ˆ Learning rate scheduling

## ğŸ¤ Contributing

We welcome contributions! ğŸ‰

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch
3. ğŸ’» Make your changes
4. ğŸ§ª Add tests if needed
5. ğŸ“ Submit a pull request

## ğŸ“š References

- [CS565600 Deep Learning](https://nthu-datalab.github.io/ml/index.html), National Tsing Hua University
- [Building a Neural Network from Scratch: Part 1](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/)
- [Building a Neural Network from Scratch: Part 2](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/)
- [Neural networks from scratch](https://developer.ibm.com/technologies/artificial-intelligence/articles/neural-networks-from-scratch), IBM Developer
- [The Softmax Function Derivative (Part 1)](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/)

---

â­ **Star this repository if you found it helpful!** â­
